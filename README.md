#  Breast Cancer Classification and Segmentation

## Overview

This project leverages advanced deep learning techniques, specifically Vision Transformers (ViTs) and UNetR architectures, to classify and segment breast cancer from ultrasound images. The solution utilizes the Breast Ultrasound Images (BUSI) dataset from Baheya Hospital, Cairo, Egypt.

## Table of Contents

- [Overview](#overview)
- [Project Workflow](#project-workflow)
- [Dataset](#dataset)
- [Vision Transformers](#vision-transformers)
- [UNetR for Segmentation](#unetr-for-segmentation)
- [Model Results](#model-results)
- [Usage](#usage)
- [Contributing](#contributing)


## Project Workflow

1. **Data Acquisition**: Downloaded the BUSI dataset from [Kaggle](https://www.kaggle.com/datasets/aryashah2k/breast-ultrasound-images-dataset).
2. **Data Preprocessing**: Performed data augmentation and preprocessing on the ultrasound images.
3. **Model Training**: Implemented and trained models using Vision Transformers for classification and UNetR for segmentation.
4. **Evaluation**: Assessed model performance and validated results using metrics and visualizations.
5. **Deployment**: Integrated the trained models into a MERN stack application for real-time breast cancer detection and segmentation.

## Dataset

- **Name**: Breast Ultrasound Images (BUSI) Dataset
- **Source**: Baheya Hospital for Early Detection & Treatment of Women's Cancer, Cairo, Egypt
- **Accessibility**: [Kaggle](https://www.kaggle.com/datasets/aryashah2k/breast-ultrasound-images-dataset)
- **Description**: The dataset contains ultrasound images and corresponding masks. It is divided into three classes: normal, benign, and malignant.

## Vision Transformers

### Vision Transformer for Classification

Vision Transformers (ViTs) are used to classify the ultrasound images into the specified categories. The `ViT_B_16` model, a pretrained transformer, was used for this task.

- **Architecture**: Vision Transformer (ViT)
- **Pretrained Model**: ViT_B_16
- **Training Accuracy**: High accuracy achieved on the classification task.
- **Prediction**: Efficient in distinguishing between normal, benign, and malignant ultrasound images.

## UNetR for Segmentation

### UNetR Architecture

UNetR is employed for the segmentation of breast cancer in ultrasound images. The model architecture comprises:

- **Transformer Encoder**: Extracts features from the input images.
- **CNN Decoder**: Decodes the features to generate segmentation masks.

#### Model Components

- **Linear Layer**: For linear transformations.
- **Convolution Layer**: For feature extraction.
- **Deconvolution Layer**: For upsampling and reconstruction of the image.

### Segmentation Workflow

1. **Input Images**: Ultrasound images of breast cancer.
2. **Actual Masks**: Ground truth masks for segmentation.
3. **Predicted Masks**: Generated by the UNetR model.

### Results and Performance

The UNetR model demonstrated robust performance in segmenting the breast cancer regions from ultrasound images. The workflow involved:

- **Data Augmentation**: Enhanced the training data by upsampling and other augmentation techniques.
- **Passes**: Multiple passes through the model for refining segmentation.

## Model Results

### Classification Results

The Vision Transformer model achieved significant accuracy in classifying the ultrasound images into the correct categories.

### Segmentation Results

The UNetR model produced detailed segmentation masks closely aligned with the actual masks. The results are visualized below:

- **Input**: Original ultrasound images.
- **Actual**: Ground truth segmentation masks.
- **Predicted**: Segmentation masks generated by the UNetR model.

## Usage

To use this project, follow these steps:

1. Clone the repository:
   ```bash
   git clone https://github.com/Bharadwajreddy1406/Classification-of-breast-cancer-tumor-using-vision-transformer.git

2. Navigate to the project directory:
   ```bash
   cd Classification-of-breast-cancer-tumor-using-vision-transformer

3. Install the necessary dependencies:
   ```bash
   pip install -r requirements.txt
4. Download the BUSI dataset from Kaggle and place it in the data directory.
5. Run the training scripts to train the models on the dataset.
6. Use the provided scripts to evaluate the models and visualize the results.

## Contributing

We welcome contributions to improve the project. Please follow these steps to contribute:

1. **Fork the repository**: Click the "Fork" button on the top right of the repository page to create a copy of the repository under your GitHub account.
2. **Create a new branch**: Open your terminal or command prompt and navigate to the project directory. Then, create a new branch for your feature or bug fix:
   ```bash
   git checkout -b feature-branch
